### 自己总结
1. embedding，嵌入，表示数据到向量空间的映射过程。
2. 作用：（1）降维，高维稀疏表征不利于计算，并且占用内存更大。（2）语义表征。具有语义相似度。（3）泛化能力：未见过的数据也可以表示。
3. 应用场景包括：NLP、CV、推荐系统等（用户emb、item emb等）。

---

在机器学习中，**embedding（嵌入）** 是一种将高维离散数据（如文本、图像、用户行为等）映射到低维连续向量空间的技术，其核心目的是通过数学表示捕捉数据的语义、结构或关联特征，使相似或相关的数据在向量空间中距离更近。以下是详细解释：

### **1. 含义解析**
- **本质**：Embedding 是一种特征工程方法，将原始数据（如单词、句子、图像块）转换为稠密向量（通常维度较低，如50-300维），同时保留数据间的语义或结构关系。
- **作用**：
  - **降维**：将高维稀疏数据（如独热编码）压缩为低维稠密向量，减少计算复杂度。
  - **语义表示**：使相似数据在向量空间中距离接近（如“猫”和“狗”的向量比“猫”和“汽车”更接近）。
  - **泛化能力**：通过连续向量表示，模型能更好地处理未见过的数据（如新词、新用户行为）。

### **2. 常见应用场景**
- **自然语言处理（NLP）**：
  - **词嵌入（Word Embedding）**：如Word2Vec、GloVe，将单词映射为向量，捕捉词义（如“king”与“queen”的向量关系）。
  - **句子/文档嵌入**：如BERT、Sentence-BERT，将整句或文档转换为向量，用于文本分类、信息检索。
- **计算机视觉（CV）**：
  - **图像嵌入**：如ResNet、VGG提取的图像特征向量，用于图像检索、相似度计算。
- **推荐系统**：
  - **用户/物品嵌入**：将用户行为或物品属性映射为向量，计算用户-物品相似度（如YouTube推荐系统）。
- **图神经网络（GNN）**：
  - **节点嵌入**：将图中的节点（如社交网络中的用户）映射为向量，捕捉节点间的关系（如社区发现）。

### **3. 中文翻译**
- **嵌入**：最常用的翻译，直接对应英文“embedding”，强调将数据“嵌入”到向量空间。
- **嵌入表示**：更强调结果（向量）的表示性质，如“词嵌入表示”。
- **嵌入向量**：特指通过嵌入技术生成的向量，如“用户嵌入向量”。

### **4. 核心特点**
- **稠密性**：与稀疏的独热编码不同，嵌入向量是稠密的，每个维度均携带信息。
- **连续性**：向量空间是连续的，支持数学运算（如加法、距离计算）。
- **可学习性**：嵌入通常通过神经网络（如自编码器、Transformer）从数据中自动学习，而非手动设计。

### **5. 示例**
- **词嵌入**：
  - 输入：“猫” → 输出：向量 `[0.2, -0.5, 0.8, ...]`（300维）
  - 语义关系：`vector("king") - vector("man") + vector("woman") ≈ vector("queen")`
- **图像嵌入**：
  - 输入：一张猫的图片 → 输出：向量 `[0.1, 0.9, -0.3, ...]`（2048维）
  - 相似性：两张猫的图片向量距离比猫与狗的图片向量距离更近。

### **总结**
Embedding 是机器学习中将离散数据转换为连续向量的核心技术，通过数学表示捕捉数据间的语义或结构关系，广泛应用于NLP、CV、推荐系统等领域。其中文翻译为“嵌入”，强调数据到向量空间的映射过程。
