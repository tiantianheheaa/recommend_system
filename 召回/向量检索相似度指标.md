在向量检索（Vector Search）中，**相似度计算（Similarity Metrics）** 是核心灵魂。它决定了系统如何理解“两个向量是否相关”。

选择错误的相似度指标，会导致检索结果完全偏离业务预期（例如：把语义完全不同的文本匹配在一起）。

下面我将从**数学定义、几何意义、适用场景、优缺点**以及**如何选择**这几个维度，对主流及边缘的相似度计算指标进行全方位解析。

---

### 一、 核心概念：距离 vs 相似度

在开始之前，必须区分两个概念：
1.  **距离 (Distance)**：值越小，越相似（如：欧氏距离）。
2.  **相似度 (Similarity)**：值越大，越相似（如：余弦相似度）。

**注意**：很多算法（如 Faiss）内部统一使用“距离”进行排序，因此如果使用相似度指标，通常需要进行转换（如 `距离 = 1 - 相似度` 或 `距离 = -相似度`）。

---

### 二、 主流相似度/距离指标详解

#### 1. 欧氏距离 (Euclidean Distance / L2 Norm)

这是最直观、最物理的距离定义。

*   **数学定义**：
    $$ d(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2} $$
*   **几何意义**：在 n 维空间中，两点之间的**直线距离**。
*   **对向量的敏感度**：对**幅值（Magnitude，即向量的长度）**和**方向**都敏感。
*   **适用场景**：
    *   **低维空间数据**：如地理坐标（经纬度）、图像像素特征、物理传感器数据。
    *   **K-Means 聚类**：K-Means 的损失函数基于 L2 距离。
    *   **向量已归一化**：如果所有向量都被归一化到单位球面上（模长为 1），L2 距离与余弦距离是单调相关的（L2 越小，余弦相似度越大）。
*   **优点**：
    *   物理意义直观，符合人类对“距离”的认知。
    *   数学性质好，可微，便于优化。
*   **缺点**：
    *   **维度灾难**：在高维空间（如 768 维的 BERT 向量），所有点之间的距离趋向于一致，区分度下降。
    *   **受长度影响大**：如果一个文档很长，其 Embedding 的模长通常较大，即使语义不相关，L2 距离也可能很大。

#### 2. 余弦相似度 (Cosine Similarity)

这是 NLP 和推荐系统领域**最常用**的指标。

*   **数学定义**：
    $$ \text{sim}(x, y) = \frac{x \cdot y}{||x|| \times ||y||} = \cos(\theta) $$
    其中 $\theta$ 是两向量之间的夹角。
*   **几何意义**：衡量两个向量在**方向上的一致性**，完全忽略向量的长度（模）。
*   **取值范围**：$[-1, 1]$。1 表示方向完全相同；-1 表示方向完全相反；0 表示正交（无关）。
*   **适用场景**：
    *   **高维稀疏数据**：如 TF-IDF 向量、One-Hot 编码。
    *   **文本语义匹配**：句子的长度不影响语义，"我爱你" 和 "我非常爱你" 应该很相似，尽管词频不同（模长不同）。
    *   **人脸识别/图像检索**：关注特征方向而非亮度/对比度（幅值）。
*   **优点**：
    *   **抗幅值干扰**：完美解决了“长文档 vs 短文档”的模长偏差问题。
    *   **高维空间效果好**：在高维空间中，方向比距离更能代表语义差异。
*   **缺点**：
    *   忽略了模长信息，而在某些场景下（如用户活跃度、物品热度），模长是有意义的。
    *   计算成本略高于点积（需要开方做归一化）。

#### 3. 点积 / 内积 (Dot Product / Inner Product)

在深度学习模型（如双塔模型）的最后一层，通常直接优化点积。

*   **数学定义**：
    $$ x \cdot y = \sum_{i=1}^{n} x_i y_i $$
*   **几何意义**：等于 $|x| |y| \cos(\theta)$。它同时包含了**方向**和**模长**的信息。
*   **与余弦的关系**：
    *   如果向量已归一化（$|x|=1, |y|=1$），则 **点积 = 余弦相似度**。
    *   如果未归一化，点积会偏向模长更大的向量。
*   **适用场景**：
    *   **协同过滤/推荐系统**：用户向量和物品向量的点积。模长可以隐式代表“用户活跃度”或“物品流行度”。活跃用户与热门物品天然点积更大，这符合推荐逻辑。
    *   **Faiss 的 `METRIC_INNER_PRODUCT`**：Faiss 中使用此指标时，它会寻找点积**最大**的向量（即最大内积搜索 MIPS）。
*   **优点**：
    *   **计算极快**：不需要开方和除法，SIMD 指令优化极好。
    *   **包含幅值信息**：能捕捉向量长度的语义。
*   **缺点**：
    *   未归一化时，容易导致“热门物品偏差”（Popularity Bias），即热门物品总是排在前面，哪怕语义不匹配。

#### 4. 曼哈顿距离 (Manhattan Distance / L1 Norm)

*   **数学定义**：
    $$ d(x, y) = \sum_{i=1}^{n} |x_i - y_i| $$
*   **几何意义**：在网格状路径中（如城市街区），从一点到另一点的距离。
*   **适用场景**：
    *   **稀疏向量**：在 L1 正则化或某些特定的稀疏编码场景下，L1 比 L2 更能产生稀疏解（即让不重要的维度变为 0）。
    *   **计算机视觉**：某些像素差异计算。
*   **优点**：计算简单（只有加减法），对异常值（Outliers）的鲁棒性比 L2 强一点（因为没有平方项）。
*   **缺点**：在高维稠密向量中很少使用，通常不如 L2 或余弦效果好。

#### 5. 切比雪夫距离 (Chebyshev Distance / L∞ Norm)

*   **数学定义**：
    $$ d(x, y) = \max_i (|x_i - y_i|) $$
*   **几何意义**：各个坐标轴方向上差值的最大值。类似于国际象棋中“王”的走法。
*   **适用场景**：
    *   **物流/仓储路径规划**：移动时间由最慢的那个轴决定。
    *   **游戏开发**：网格地图中的移动成本。
    *   **向量检索中极少使用**，通常只在特定工业控制场景出现。

#### 6. 汉明距离 (Hamming Distance)

*   **数学定义**：两个等长二进制串对应位不同的数量。
*   **适用场景**：
    *   **二值向量（Binary Vectors）**：如 LSH 哈希码、ORB 图像特征描述子。
    *   **极低内存消耗**：一个 float32 向量（512维）占 2KB，而二值向量（512位）只占 64 字节。
*   **优点**：计算极快（XOR + Popcount，CPU 单指令周期完成）。
*   **缺点**：信息损失大，精度通常不如浮点向量。

#### 7. 杰卡德相似度 (Jaccard Similarity)

*   **数学定义**：
    $$ J(A, B) = \frac{|A \cap B|}{|A \cup B|} $$
*   **适用场景**：
    *   **集合数据**：如标签匹配、关键词重叠。
    *   **二值向量**：可以看作是二值向量的归一化点积。
*   **优点**：对集合大小的差异不敏感，适合比较文档的词汇重叠率。
*   **缺点**：不适合稠密语义向量。

---

### 三、 进阶与特殊指标

#### 1. KL 散度 (Kullback-Leibler Divergence)
*   **定义**：衡量两个概率分布 $P$ 和 $Q$ 的差异。
*   **场景**：通常用于生成模型（如 VAE）、话题模型（LDA）。在向量检索中较少直接作为距离 metric，更多用于损失函数。

#### 2. 马氏距离 (Mahalanobis Distance)
*   **定义**：考虑了特征之间的相关性（协方差矩阵）。
*   **场景**：特征之间存在强相关性且量纲不一时（如身高和体重）。在度量学习（Metric Learning）中常用。

#### 3. BM25 (稀疏向量专用)
*   **注意**：BM25 不是向量距离，而是基于词频的打分函数。
*   **场景**：传统搜索引擎的核心。在现代混合搜索（Hybrid Search）中，常将 BM25（稀疏）与 Cosine（稠密）分数加权求和（如 Reciprocal Rank Fusion）。

---

### 四、 全面对比与选择指南

#### 1. 对比表格

| 指标名称 | 类型 | 核心关注点 | 对模长敏感？ | 计算复杂度 | 典型应用场景 |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **欧氏距离 (L2)** | 距离 | 空间位置 | **是** | 中 (开方) | 图像特征、地理位置、归一化后的语义 |
| **余弦相似度** | 相似度 | 方向/角度 | **否** | 高 (归一化) | **文本语义**、人脸识别、高维 Embedding |
| **点积 (IP)** | 相似度 | 方向 + 模长 | **是** | **低** | **推荐系统**、最大内积搜索(MIPS) |
| **曼哈顿 (L1)** | 距离 | 轴向差值 | 是 | 低 | 稀疏编码、路径规划 |
| **汉明距离** | 距离 | 比特位差异 | 否 (二进制) | **极低** | 二值向量、去重、快速过滤 |

#### 2. 核心选择逻辑：为什么 NLP 偏爱 Cosine？

假设有两个句子：
*   A: "苹果"
*   B: "苹果 苹果 苹果 苹果 苹果"（重复5次）

如果用 One-Hot 或 TF-IDF 向量：
*   A 的向量模长小。
*   B 的向量模长大（词频高）。
*   **L2 距离**：A 和 B 距离很远（因为幅值差异大）。
*   **点积**：B 和 B 自己的点积巨大，可能导致排序偏差。
*   **余弦相似度**：A 和 B 方向几乎一致（都指向“苹果”这个语义），相似度接近 1。**这正是我们想要的语义匹配。**

#### 3. 工业界最佳实践

1.  **默认选择**：对于 **BERT/RoBERTa** 等生成的稠密向量（Dense Vector），**余弦相似度**是首选。
2.  **性能优化**：如果追求极致性能，先将向量**归一化**（Normalize），然后使用**点积**。
    *   原因：归一化后，点积等价于余弦相似度，但计算更快（Faiss 中的 `IndexFlatIP` 比 `IndexFlatL2` 快）。
3.  **推荐系统**：直接使用**点积**（不归一化），因为向量的模长可以隐式学习出用户/物品的偏置（Bias）。
4.  **混合搜索 (Hybrid Search)**：
    *   不要只用一种！现代搜索系统通常同时计算 `Cosine(Dense Vector)` 和 `BM25(Sparse Vector)`，然后通过加权（如 0.7 * Cosine + 0.3 * BM25）或 RRF 融合结果，兼顾语义匹配和关键词精确匹配。
5.  **二值化场景**：如果内存极其紧张（如嵌入式设备），使用 **汉明距离**，但需要专门的二值化模型（如 BGE-BGE-M3 或 LSH）。

### 五、 Faiss 中的参数对应

在使用 Faiss 时，你需要根据选择的指标设置 `metric` 参数：

```python
# 1. 欧氏距离
index = faiss.IndexFlatL2(d)

# 2. 余弦相似度 (需要先归一化向量，然后用内积)
xb_norm = xb / np.linalg.norm(xb, axis=1, keepdims=True)
index = faiss.IndexFlatIP(d) # IP = Inner Product

# 3. 直接用点积 (不归一化)
index = faiss.IndexFlatIP(d)
```

**总结**：
*   做**语义搜索**（文本、图、音）：首选 **余弦相似度**（归一化后用点积加速）。
*   做**推荐系统**（双塔模型）：首选 **点积**。
*   做**图像/坐标**检索：首选 **欧氏距离**。
*   做**去重/极速过滤**：首选 **汉明距离**。
